{
  "hash": "09e6cf6edfc99d284f172d1517090a85",
  "result": {
    "markdown": "---\ntitle: \"인하대학교 강간살인 사건 언론 재현 분석\"\nsubtitle: \"1. Clustering analysis을 중심으로\"\nauthor: \"sorrychoe\"\nformat: \n  html: \n    smooth-scroll: true\neditor: visual\nexecute: \n  echo: true\n  eval: true\n---\n\n\n\n## 개요\n본 내용은 인하대학교 사건 기사를 분석한 내용을 담고 있다.\n\n먼저, 가장 보편적으로 기사 분석에 가장 많이 사용하는 방법인 Clustering Analysis 기법을 사용하여, 분석을 진행했다.\n\n분석 언어는 Python을 사용했으며, 군집 분석은 Scikit-Learn의 KMeans() 함수를 활용하여 분석을 진행했다.\n\n깊게 존재하는 의미 연결망 분석 및 담론 분석은 이후 보고서에서 작성하였다.\n\n## 1. 데이터 현황 및 필터링\n\n먼저 수집한 기사들의 언론사 구성 빈도는 다음과 같다.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npress = bkp.press_counter(news_df)\nsns.barplot(data = press, x = \"기사\", y = \"언론사\")\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n<Axes: xlabel='기사', ylabel='언론사'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-3-output-2.png){width=707 height=659}\n:::\n:::\n\n\n<br/>\n\n해당 데이터 중 포토 기사는 내용적 측면에서 데이터 분석이 매우 어렵다. 이에 포토 기사는 본 분석에서 제외하기로 결정했다.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfilt = news_df[news_df['제목'].str.contains('포토')].index\nnews_df.drop(filt, inplace=True)\nnews_df.reset_index(drop = True, inplace = True)\n```\n:::\n\n\n## 2. 키워드 빈도 분석\n\n키워드 빈도 분석을 진행하였다. 단어 전체의 키워드를 추출한 후, 이를 워드클라우드로 변환하여 분석하였다.\n\n분석 결과는 다음과 같다.\n\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nwc = WordCloud(font_path = './data_analysis/NanumBarunGothic.ttf',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(key_words.set_index('단어').to_dict()[\"빈도\"])\n\n\nplt.figure(figsize = (8, 8))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-6-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n다음은 언론사 별 단어 빈도 분석 결과이다. 본 분석은 보수 언론(조선, 중앙, 동아)와 진보 언론(한겨례, 경향) 간의 차이를 확인하고자 진행하였다.\n\n분석은 위의 절차와 동일한 방법으로 키워드 변환 이후 워드클라우드로 시각화했다.\n\n분석 결과는 다음과 같다.\n\n**조선일보**\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nbkp.press_keywords_wordcloud(news_df, '조선일보')\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-7-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n**중앙일보**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nbkp.press_keywords_wordcloud(news_df, '중앙일보')\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-8-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n**동아일보**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nbkp.press_keywords_wordcloud(news_df, '동아일보')\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-9-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n**한겨례**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nbkp.press_keywords_wordcloud(news_df, '한겨레')\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-10-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n**경향신문**\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nbkp.press_keywords_wordcloud(news_df, '경향신문')\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-11-output-1.png){width=611 height=611}\n:::\n:::\n\n\n<hr/>\n\n## DTM 구축\n\n우선 문서 데이터의 분석을 원활히 하기 위해 DTM(Document Term Matrix)을 구축하였다.\n\nDTM은 문서에서 키워드만 추출하여 list에 담은 뒤, Scikit-Learn의 CounterVCtorizer를 활용하여 word vector를 생성하였다. 그 후, TfidfTransformer를 통해 문서 내 단어 중요도를 반영하였다.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\ntext = news_df['키워드']\n\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n])        \nvec = pipeline.fit_transform(text).toarray()\n```\n:::\n\n\nDTM 생성 후, 모델에서의 분석 정확도 고려를 위해, Nomalizer로 스케일링을 진행하였다.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import Normalizer\n\nnor = Normalizer()\nnorvec = nor.fit_transform(vec)\n```\n:::\n\n\n다음은 DTM을 t-SNE기법으로 차원을 축소하여 시각화한 형태이다.\n\n전반적으로 문서가 흩어져 있는 양상을 보여, 군집 간의 밀집 정도 파악은 다소 어려워보인다. 아무래도 문서의 양이 절대적으로 적다보니, 문서 간 군집 형성 정도를 확인하기 어려운 부분이 있다.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, learning_rate=400).fit_transform(norvec)\n\ntsne_df = pd.DataFrame(tsne, columns = ['component 0', 'component 1'])\n\nplt.scatter(tsne_df['component 0'], tsne_df['component 1'], color = 'orange')\n\nplt.xlabel('component 0')\nplt.ylabel('component 1')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-14-output-2.png){width=671 height=659}\n:::\n:::\n\n\n<hr/>\n\n## K-means Clustering\n\n본격적으로 데이터 분석을 진행하고자 문서 군집화를 진행하였다.\n\n방법의 경우, K-means Clustering으로 진행하였다. 문서의 양이 많지 않은 데다가 분포가 고르지 못한 경향이 있어, K-means Clustering이 이에 가장 적합한 분석 방법이라고 판단했다.\n\n먼저, K-Means Clustering을 시행하기 위해 최적 군집 갯수를 유추하였다. Elbow method를 통해 분석을 진행하였으며, 이를 통해 최적 군집의 갯수는 12개로 유추하였다.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvzr = KElbowVisualizer(KMeans(max_iter=1000, random_state=10), k=(2, 20))\nvzr.fit(norvec)\nvzr.poof()\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-15-output-1.png){width=720 height=486}\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'>\n```\n:::\n:::\n\n\n최적 군집 갯수를 기점으로 실루엣 계수를 분석하였다.\n\n실루엣 계수는 군집 간의 유사도를 수치적으로 확인하는 방법이다.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nkmeans= KMeans(n_clusters=12, max_iter=1000, random_state=10) #최적 Topic 개수 12개를 기점으로 진행\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\nvisualizer.fit(norvec)\nvisualizer.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](inha_clustering_files/figure-html/cell-16-output-1.png){width=667 height=486}\n:::\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<Axes: title={'center': 'Silhouette Plot of KMeans Clustering for 635 Samples in 12 Centers'}, xlabel='silhouette coefficient values', ylabel='cluster label'>\n```\n:::\n:::\n\n\n전반적으로 계수가 높지 않았다. 다만, 텍스트 데이터 특성상 실루엣 계수가 낮아도 의미적으로는 문제가 없는 경우가 다소 있어,'\nk 값은 elbow method의 결과인 12로 진행하기로 결정하였다. \n\n<br/>\n\n\n군집 별 기사 갯수는 다음과 같다.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nlabels = kmeans.labels_\n\ntopic_df = news_df[['언론사', '제목', '키워드']]\n\ntopic_df['군집'] = labels\n\ntopic_df.groupby('군집').size()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n군집\n0      34\n1      14\n2      59\n3      17\n4     103\n5      75\n6      42\n7      40\n8     186\n9      29\n10     16\n11     20\ndtype: int64\n```\n:::\n:::\n\n\n<br/>\n\n\n군집 내 기사들의 내용을 분석한 결과, 다음과 같이 클러스터링이 진행됐다.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ntopic_df.loc[topic_df['군집']==0, '군집'] = '경제'\ntopic_df.loc[topic_df['군집']==1, '군집'] = '리멤버 0715'\ntopic_df.loc[topic_df['군집']==2, '군집'] = '인하대 사건 그 이후'\ntopic_df.loc[topic_df['군집']==3, '군집'] = '인하대 입시'\ntopic_df.loc[topic_df['군집']==4, '군집'] = '인하대 사건 조사보도'\ntopic_df.loc[topic_df['군집']==5, '군집'] = '가해자 재판'\ntopic_df.loc[topic_df['군집']==6, '군집'] = '젠더 이슈'\ntopic_df.loc[topic_df['군집']==7, '군집'] = '가해자 체포'\ntopic_df.loc[topic_df['군집']==8, '군집'] = '관련 없는 기사'\ntopic_df.loc[topic_df['군집']==9, '군집'] = '학교 측 가해자 징계'\ntopic_df.loc[topic_df['군집']==10, '군집'] = '인하대 총장'\ntopic_df.loc[topic_df['군집']==11, '군집'] = '부고'\n```\n:::\n\n\n다음 결과 중 본 사건과 관련 없는 기사는 별도로 필터링한 상태로, 데이터를 추출하였다.\n\n해당 데이터를 통해 의미 연결망 분석을 진행하였으며, 해당 내용은 \"2. 의미연결망 분석을 중심으로\" 편에서 다루기로 한다.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfilter_list = ['경제', '인하대 입시', '관련 없는 기사', '인하대 총장', '부고']\ninha_df = topic_df[~topic_df['군집'].isin(filter_list)]\ninha_df.reset_index(drop = True, inplace = True)\n```\n:::\n\n\n",
    "supporting": [
      "inha_clustering_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}