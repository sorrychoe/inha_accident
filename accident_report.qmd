---
title: "2. 의미연결망 분석을 중심으로"
author: "sorrychoe"
format: 
  html: 
    smooth-scroll: true
editor: visual
execute: 
  echo: true
  eval: true
---

```{r setup, include=F, message=F}
library(tidyverse)
library(plotly)
library(tidytext)
library(tidylo)
library(gt) 
library(igraph)
library(quanteda)
library(quanteda.textplots)
library(readxl)


inha <- read_excel("./data_analysis/inha_topic.xlsx")
```

<hr/>

## 개요
본 문서는 1편에서 진행한 k-means clustering method로 추출한 기사를 군집 별 데이터 분석 및 단어 빈도 분석, 의미연결망 분석을
진행한 내용을 담고 있으며, 분석 도구는 R로 진행하였다. 아무래도 의미연결망 분석이나 빈도 시각화에 있어선 아직까지
Python보다는 R이 더 우수한 경향이 있기에, R로 변경하여 분석을 이어갔다.

분석에 사용된 총 기사의 갯수는 다음과 같다.

```{r, echo = F}
dim(inha)
```

<hr/>

# 단어 분석

먼저 기사에 사용된 단어를 분석한 결과, 총 4190개로 나타났다.

```{r, echo = F}
inha %>% 
  select(키워드) %>% 
  rowid_to_column() %>%
  unnest_tokens(
    input = "키워드",
    output = "단어") %>% 
  filter(nchar(단어) >= 2) %>% 
  count(`단어`, sort = TRUE) -> words

dim(words) #4190
```

상위 15개의 주요 단어를 추출한 결과는 다음과 같다.

```{r, echo = F}
words %>% head(15) #counter
```

<hr/>

## 보도 빈도 분석

언론사 별 보도 빈도는 다음과 같다.

```{r}
inha %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% gt() %>% tab_header('언론사 별 보도 빈도') %>% 
  cols_label(언론사 = "언론사", n = '빈도')
```

보도 빈도만 두고 본 결과, 3개월의 기간 동안 해당 어젠다를 중점적으로 다룬 언론사는 세계일보였다. 그 외에는 전반적으로 비슷한 수준의 보도를 진행하였다.

<hr/>

## 군집 별 보도 분석

이전에 기사 필터링을 진행하여 나타난 군집들의 양상을 근거로 언론사들의 보도 스탠스를 유추하여 보았다.

군집 별 기사 빈도는 다음과 같다.

```{r}
inha %>% 
  group_by(군집) %>% 
  tally() %>% 
  arrange(desc(n)) %>% gt() %>% tab_header('군집 별 기사 빈도') %>% 
  cols_label(군집 = "군집", n = '빈도')
```

이들 중 언론사의 어젠다 설정 방향을 유추할 수 있는 군집인 "인하대 사건 조사보도", "인하대 사건 그 이후", "젠더 이슈"를 추출하여 언론사 별 보도 비율을 확인하였다.

먼저 인하대 사건 조사보도 군집에 해당하는 기사들의 언론사 구성 비율이다

```{r}
inha %>% 
  filter(군집 == '인하대 사건 조사보도') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.6),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")
```

그 다음 인하대 사건 그 이후 군집에 해당하는 기사들의 언론사 구성 비율이다

```{r}
inha %>% 
  filter(군집 == '인하대 사건 그 이후') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.5),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")

```

마지막으로 젠더 이슈에 대한 기사들의 언론사 구성 비율이다.

```{r}
inha %>% 
  filter(군집 == '젠더 이슈') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>%
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.5),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")
```

<hr/>

## 언론사 별 단어 분석

언론사 별로 주로 사용한 단어는 다음과 같다.

```{r, echo = F}
inha %>% 
  select(언론사, 키워드) %>% 
  rowid_to_column() %>%
  unnest_tokens(
    input = "키워드",
    output = "단어") %>% 
  group_by(언론사) %>% 
  filter(nchar(단어) >= 2) %>% 
  count(`단어`, sort = TRUE)-> inha.words

reorder_within <- function(x, by, within, fun = sum, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}
```

```{r}
inha.words %>% 
  slice_max(n, n = 10) %>%
  ggplot(aes(x = n, y = reorder_within(단어, n, 언론사), fill = 언론사)) +
  geom_col(show.legend = F) +
  facet_wrap(~ 언론사, scales = 'free') + 
  scale_y_reordered()+
  xlab("")+
  ylab("")
```

tf-idf를 활용하여 단어 별 상대 빈도를 도출했다.

상대 빈도 분석 결과는 다음과 같다.

```{r, echo = F, message=F}

inha.words %>%
  bind_tf_idf(document = 언론사, term = 단어, n = n) %>% 
  filter(grepl("[가-힣]", 단어)) %>% 
  mutate(score = round(tf_idf * 1000, 3)) %>% 
  arrange(-score) -> tfidf.df
```

```{r}
tfidf.df %>% 
  slice_max(score, n = 5) %>%
  ggplot(aes(x = score, y = reorder_within(단어, score, 언론사), fill = 언론사)) +
  geom_col(show.legend = F) +
  facet_wrap(~ 언론사, scales = 'free') + 
  scale_y_reordered()+
  xlab("")+
  ylab("")
```

<hr/>

## Press Network Analysis

먼저, 언론사 간 네트워크 분석을 실시했다.

본 분석은 tf-idf로 상대 빈도 분석을 진행한 데이터를 기준으로, 공동으로 등장하는 단어에 기반하여 네트워크 분석을 실시했다.

분석 결과의 정확성 확보를 위해 한국어가 아닌 단어들(영문, 숫자 등)은 분석에서 제외했다.

```{r, include=F}
tfidf.df %>%
  filter(grepl("[가-힣]", 단어)) %>% 
  group_by(언론사) %>%
  slice_max(score, n = 15) -> dtm.df
```

또한 네트워크 분석과 함께 군집화를 진행했다. 문서 간의 유사도가 높은 네트워크를 중심으로 군집을 형성했다. 유사도 판정 기준은 분석에 사용한 프로그래밍 언어인 R의 유사도 검정 기능을 활용했다.

네트워크 분석 결과는 다음과 같다.

```{r, echo = F}
dtm <- t(table(dtm.df[1:2]))
press <- crossprod(dtm)
diag(press) <- 0
dtm <- as.matrix(press)

press.nt <- graph_from_adjacency_matrix(dtm, mode="undirected", weighted=TRUE)
clust <- cluster_walktrap(press.nt)
```

```{r}
plot(clust, press.nt)
```

<hr/>

## Sementic Network Analysis

마지막으로 단어 간의 의미 연결망 분석을 실시했다. 네트워크 분석의 경우, 데이터를 통해 동시 출현 단어 행렬을 형성하고, 이를 토대로 quenteda를 활용하여 네트워크 분석을 시도했다.

분석 결과는 다음과 같다.

```{r, echo=F}
options(ggrepel.max.overlaps = 19)

com <- table(dtm.df[1:2])
com <- crossprod(com)
diag(com) <- 0
com<- as.data.frame(com)

dfm <- as.dfm(com)
fc<- fcm(dfm)

size = log(colSums(fc)) / max(log(colSums(fc))) * 5
```

```{r}
#| warning: false

textplot_network(fc, 
                 min_freq = 2, 
                 edge_alpha = 0.5, 
                 edge_color = "blue",
                 vertex_size = size,
                 edge_size = 0.5)
```
