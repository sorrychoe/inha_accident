---
title: "인하대학교 강간살인 사건 언론 재현 분석"
subtitle: "빈도 분석 및 의미연결망 분석"
author: "sorrychoe"
format: 
  html: 
    smooth-scroll: true
editor: visual
execute: 
  echo: true
  eval: true
---

```{r setup, include=F, message=F}
library(tidyverse)
library(plotly)
library(tidytext)
library(tidylo)
library(gt) 
library(igraph)
library(quanteda)
library(quanteda.textplots)
library(readxl)


inha <- read_excel("./data_analysis/inha_topic.xlsx")
```

<hr/>

## 개요

지난 2022년 7월 15일, 인하대학교 재학생 강간살인 사건의 언론 재현 수준을 분석한 보고서이다. 

먼저 해당 사건 발생일 기준 3개월 간의 뉴스 기사 데이터를 수집하였다.해당 기간 이후의 기사는 
대체적으로 본 사건과 관련이 없는 기사가 많았다. 이에, 총 3개월 치의 기사만 수집하였다. 

기사는 빅카인즈(BigKinds)를 통해 수집하였다. 포털 사이트의 기사를 크롤링을 하는 방법도 있으나,
포털 뉴스 특성상 기사가 중복된 내용이거나 지나친 편향성을 지닌 기사가 많아, 본 분석에선 빅카인즈에서 데이터를 별도로 수집하는 방식을
선택했다.

또한 언론의 어젠다를 분석하는데 있어 주요 신문사들의 보도를 보는 것이 좋다고 판단하여 총 10개의 언론사("세계일보", "문화일보", "경향신문", "동아일보", "한겨레", "서울신문","한국일보","중앙일보", "국민일보","조선일보")의 기사를 선별하여 데이터를 수집했다.

추가로, 7월 15일부터 10월 15일의 기간에 보도된 기사 중, 해당 사건과 관련 없는 기사를 필터링하였다. 
기사 필터링은 Python 환경에서 진행하였으며, 해당 내용 자체만으로도 내용이 많아, 별도의 문서에서 다루기로 한다.

분석에 사용된 총 기사의 갯수는 다음과 같다.
```{r, echo = F}
dim(inha)
```

<hr/>

# 단어 분석

먼저 기사에 사용된 단어를 분석한 결과, 총 4190개로 나타났다.
```{r, echo = F}
inha %>% 
  select(키워드) %>% 
  rowid_to_column() %>%
  unnest_tokens(
    input = "키워드",
    output = "단어") %>% 
  filter(nchar(단어) >= 2) %>% 
  count(`단어`, sort = TRUE) -> words

dim(words) #4190
```

상위 15개의 주요 단어를 추출한 결과는 다음과 같다. 
```{r, echo = F}
words %>% head(15) #counter
```

<hr/>

## 보도 빈도 분석

언론사 별 보도 빈도는 다음과 같다.
```{r}
inha %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% gt() %>% tab_header('언론사 별 보도 빈도') %>% 
  cols_label(언론사 = "언론사", n = '빈도')
```

보도 빈도만 두고 본 결과, 3개월의 기간 동안 해당 어젠다를 중점적으로 다룬 언론사는 세계일보였다.
그 외에는 전반적으로 비슷한 수준의 보도를 진행하였다.

<hr/>

## 군집 별 보도 분석

이전에 기사 필터링을 진행하여 나타난 군집들의 양상을 근거로 언론사들의 보도 스탠스를 유추하여 보았다.

군집 별 기사 빈도는 다음과 같다.
```{r}
inha %>% 
  group_by(군집) %>% 
  tally() %>% 
  arrange(desc(n)) %>% gt() %>% tab_header('군집 별 기사 빈도') %>% 
  cols_label(군집 = "군집", n = '빈도')
```

이들 중 언론사의 어젠다 설정 방향을 유추할 수 있는 군집인 "인하대 사건 조사보도", "인하대 사건 그 이후", "젠더 이슈"를 추출하여
언론사 별 보도 비율을 확인하였다.

먼저 인하대 사건 조사보도 군집에 해당하는 기사들의 언론사 구성 비율이다
```{r}
inha %>% 
  filter(군집 == '인하대 사건 조사보도') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.6),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")
```

그 다음 인하대 사건 그 이후 군집에 해당하는 기사들의 언론사 구성 비율이다
```{r}
inha %>% 
  filter(군집 == '인하대 사건 그 이후') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.5),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")

```


마지막으로 젠더 이슈에 대한 기사들의 언론사 구성 비율이다.
```{r}
inha %>% 
  filter(군집 == '젠더 이슈') %>% 
  group_by(언론사) %>% 
  tally() %>% 
  arrange(desc(n)) %>%
  ggplot(aes(x = "", y = n, fill = 언론사))+
  geom_bar(width = 1, stat = 'identity', color = "white")+
  theme(axis.text.y = element_blank(),axis.ticks = element_blank(), legend.text = element_text(size = 9))+
  geom_text(aes(label = paste(round(n/sum(n)*100, 2),"%")),
            position = position_stack(vjust = 0.5),
            check_overlap = TRUE,
            color = 'white')+
  coord_polar('y', start = 0)+
  xlab("")+
  ylab("")
```

<hr/>

## 언론사 별 단어 분석

언론사 별로 주로 사용한 단어는 다음과 같다.

```{r, echo = F}
inha %>% 
  select(언론사, 키워드) %>% 
  rowid_to_column() %>%
  unnest_tokens(
    input = "키워드",
    output = "단어") %>% 
  group_by(언론사) %>% 
  filter(nchar(단어) >= 2) %>% 
  count(`단어`, sort = TRUE)-> inha.words

reorder_within <- function(x, by, within, fun = sum, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}
```

```{r}
inha.words %>% 
  slice_max(n, n = 10) %>%
  ggplot(aes(x = n, y = reorder_within(단어, n, 언론사), fill = 언론사)) +
  geom_col(show.legend = F) +
  facet_wrap(~ 언론사, scales = 'free') + 
  scale_y_reordered()+
  xlab("")+
  ylab("")
```


tf-idf를 활용하여 단어 별 상대 빈도를 도출했다.

상대 빈도 분석 결과는 다음과 같다.
```{r, echo = F, message=F}

inha.words %>%
  bind_tf_idf(document = 언론사, term = 단어, n = n) %>% 
  filter(grepl("[가-힣]", 단어)) %>% 
  mutate(score = round(tf_idf * 1000, 3)) %>% 
  arrange(-score) -> tfidf.df
```

```{r}
tfidf.df %>% 
  slice_max(score, n = 5) %>%
  ggplot(aes(x = score, y = reorder_within(단어, score, 언론사), fill = 언론사)) +
  geom_col(show.legend = F) +
  facet_wrap(~ 언론사, scales = 'free') + 
  scale_y_reordered()+
  xlab("")+
  ylab("")
```

<hr/>

## Press Network Analysis

먼저, 언론사 간 네트워크 분석을 실시했다.

본 분석은 tf-idf로 상대 빈도 분석을 진행한 데이터를 기준으로, 
공동으로 등장하는 단어에 기반하여 네트워크 분석을 실시했다.

분석 결과의 정확성 확보를 위해 한국어가 아닌 단어들(영문, 숫자 등)은 분석에서 제외했다.
```{r, include=F}
tfidf.df %>%
  filter(grepl("[가-힣]", 단어)) %>% 
  group_by(언론사) %>%
  slice_max(score, n = 15) -> dtm.df
```

또한 네트워크 분석과 함께 군집화를 진행했다. 문서 간의 유사도가 높은 네트워크를 중심으로 군집을 형성했다.
유사도 판정 기준은 분석에 사용한 프로그래밍 언어인 R의 유사도 검정 기능을 활용했다. 

네트워크 분석 결과는 다음과 같다. 
```{r, echo = F}
dtm <- t(table(dtm.df[1:2]))
press <- crossprod(dtm)
diag(press) <- 0
dtm <- as.matrix(press)

press.nt <- graph_from_adjacency_matrix(dtm, mode="undirected", weighted=TRUE)
clust <- cluster_walktrap(press.nt)
```

```{r}
plot(clust, press.nt)
```

<hr/>

## Sementic Network Analysis

마지막으로 단어 간의 의미 연결망 분석을 실시했다. 
네트워크 분석의 경우, 데이터를 통해 동시 출현 단어 행렬을 형성하고, 이를 토대로 quenteda를 활용하여 네트워크 분석을 시도했다.

분석 결과는 다음과 같다.

```{r, echo=F}
options(ggrepel.max.overlaps = 19)

com <- table(dtm.df[1:2])
com <- crossprod(com)
diag(com) <- 0
com<- as.data.frame(com)

dfm <- as.dfm(com)
fc<- fcm(dfm)

size = log(colSums(fc)) / max(log(colSums(fc))) * 5
```

```{r}
#| warning: false

textplot_network(fc, 
                 min_freq = 2, 
                 edge_alpha = 0.5, 
                 edge_color = "blue",
                 vertex_size = size,
                 edge_size = 0.5)
```